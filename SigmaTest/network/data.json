{"edges":[{"label":"","source":"0","target":"5","id":"5","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"3","id":"3","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"8","id":"8","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"1","id":"1","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"13","id":"13","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"7","id":"7","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"2","id":"2","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"6","id":"6","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"10","id":"10","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"4","id":"4","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"9","id":"9","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"12","id":"12","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"14","id":"14","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"18","id":"18","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"16","id":"16","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"17","id":"17","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"15","id":"15","attributes":{},"color":"rgb(159,145,208)","size":1.0},{"label":"","source":"0","target":"11","id":"11","attributes":{},"color":"rgb(159,145,208)","size":1.0}],"nodes":[{"label":"Spatiotemporal salient points for visual recognition of human actions","x":38.629310607910156,"y":336.5458068847656,"id":"7","attributes":{"Authors":"A Oikonomopoulos, I Patras, M Pantic","Title":"Spatiotemporal salient points for visual recognition of human actions"},"color":"rgb(63,235,219)","size":10.0},{"label":"Extracting spatiotemporal interest points using global information","x":-11.585197448730469,"y":-91.42103576660156,"id":"6","attributes":{"Authors":"S-F Wong, R Cipolla","Title":"Extracting spatiotemporal interest points using global information"},"color":"rgb(63,235,219)","size":10.0},{"label":"Learning realistic human actions from movies","x":-106.58296966552734,"y":273.8333435058594,"id":"10","attributes":{"Authors":"I Laptev, M Marszaek, C Schmid, B Rozenfeld","Title":"Learning realistic human actions from movies"},"color":"rgb(63,235,219)","size":10.0},{"label":"Discriminative object class models of appearance and shape by correlations","x":287.2989196777344,"y":232.98802185058594,"id":"14","attributes":{"Authors":"S Savarese, J Winn, A Criminisi","Title":"Discriminative object class models of appearance and shape by correlations"},"color":"rgb(63,235,219)","size":10.0},{"label":"Unsupervised learning of human action categories using spatial-temporal words","x":-112.87895965576172,"y":-241.42408752441406,"id":"5","attributes":{"Authors":"J C Nibles, H Wang, L F-F Li","Title":"Unsupervised learning of human action categories using spatial-temporal words"},"color":"rgb(63,235,219)","size":10.0},{"label":"Learning to detect objects in images via a sparse, part-based representation","x":372.9078674316406,"y":91.43125915527344,"id":"16","attributes":{"Authors":"S Agarwal, A Awan, Roth D 2004","Title":"Learning to detect objects in images via a sparse, part-based representation"},"color":"rgb(63,235,219)","size":10.0},{"label":"MoSIFT: Recognizing Human Actions in Surveillance Videos","x":156.8442840576172,"y":-185.1283416748047,"id":"0","attributes":{"Authors":"Ming-yu Chen, Alex Hauptmann","Title":"MoSIFT: Recognizing Human Actions in Surveillance Videos"},"color":"rgb(255,56,197)","size":18.0},{"label":"Space-time interest points","x":42.087867736816406,"y":-216.7896270751953,"id":"1","attributes":{"Authors":"I Laptev, T Lindeberg","Title":"Space-time interest points"},"color":"rgb(63,235,219)","size":10.0},{"label":"Using Bigrams in Text Categorization","x":-340.8488464355469,"y":526.5673828125,"id":"13","attributes":{"Authors":"R Bekkerman, J Allan","Title":"Using Bigrams in Text Categorization"},"color":"rgb(63,235,219)","size":10.0},{"label":"Distinctive image features from scale invariant key points","x":-115.2597885131836,"y":-45.08635330200195,"id":"9","attributes":{"Authors":"D G Lowe","Title":"Distinctive image features from scale invariant key points"},"color":"rgb(63,235,219)","size":10.0},{"label":"Spatial-temporal correlations for unsupervised action classification","x":102.0907974243164,"y":85.69987487792969,"id":"15","attributes":{"Authors":"S Savarese, A D Pozo, J C Niebles, F-F Li","Title":"Spatial-temporal correlations for unsupervised action classification"},"color":"rgb(63,235,219)","size":10.0},{"label":"Local features and kernels for classification of texture and object categories: A comprehensive study","x":138.7344207763672,"y":-225.77476501464844,"id":"12","attributes":{"Authors":"J Zhang, M Marszaek, S Lazebnik, C Schmid","Title":"Local features and kernels for classification of texture and object categories: A comprehensive study"},"color":"rgb(63,235,219)","size":10.0},{"label":"Informedia @ TRECVID2008: Exploring New Frontiers TRECVID Video Retrieval Evaluation Workshop","x":237.53604125976562,"y":-488.9183654785156,"id":"18","attributes":{"Authors":"A Hauptmann, R V Baron, M Chen, M Christel, W-H Lin, X Sun, V Valdes, J Yang, L Mummert, S Schlosser","Title":"Informedia @ TRECVID2008: Exploring New Frontiers TRECVID Video Retrieval Evaluation Workshop"},"color":"rgb(63,235,219)","size":10.0},{"label":"Action Snippets: How many frames does human action recognition require","x":-380.59161376953125,"y":320.7457580566406,"id":"11","attributes":{"Authors":"K Schindler, L V Gool","Title":"Action Snippets: How many frames does human action recognition require"},"color":"rgb(63,235,219)","size":10.0},{"label":"Efficient visual event detection using volumetric features","x":213.3289031982422,"y":-31.303178787231445,"id":"3","attributes":{"Authors":"Y Ke, R Sukthankar, M Hebert","Title":"Efficient visual event detection using volumetric features"},"color":"rgb(63,235,219)","size":10.0},{"label":"Institute of Standards and Technology (NIST): TRECVID 2008 Evaluation for Surveillance Event Detection","x":-7.8586955070495605,"y":-385.5386962890625,"id":"8","attributes":{"Authors":"National","Title":"Institute of Standards and Technology (NIST): TRECVID 2008 Evaluation for Surveillance Event Detection"},"color":"rgb(63,235,219)","size":10.0},{"label":"Behavior Recognition via Sparse Spatio-Temporal Features","x":47.114906311035156,"y":-268.2290344238281,"id":"4","attributes":{"Authors":"P Dollr, V Rabaud, G Gottrell, S Belongie","Title":"Behavior Recognition via Sparse Spatio-Temporal Features"},"color":"rgb(63,235,219)","size":10.0},{"label":"Recognition of Aggressive Human Behavior Using Binary Local Motion Descriptors","x":-207.5789337158203,"y":422.3644104003906,"id":"17","attributes":{"Authors":"C Chen, H Wactlar, M-Y Chen, G Can, A Bharucha, A Hauptmann","Title":"Recognition of Aggressive Human Behavior Using Binary Local Motion Descriptors"},"color":"rgb(63,235,219)","size":10.0},{"label":"Recognizing human actions: A local svm approach","x":-333.7998962402344,"y":-205.2398223876953,"id":"2","attributes":{"Authors":"C Schuldt, I Laptev, B Caputo","Title":"Recognizing human actions: A local svm approach"},"color":"rgb(63,235,219)","size":10.0}]}